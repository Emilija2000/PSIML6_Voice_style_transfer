{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT OUR LIBRARIES\n",
    "import dataloader\n",
    "import best_classif_model\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  50  items from dataset\n",
      "Loading:  100  items from dataset\n",
      "Loading:  150  items from dataset\n",
      "Loading:  200  items from dataset\n",
      "Loading:  250  items from dataset\n",
      "Loading:  300  items from dataset\n",
      "Loading:  350  items from dataset\n",
      "Loading:  400  items from dataset\n",
      "Loading:  450  items from dataset\n",
      "Loading:  500  items from dataset\n",
      "Loading:  550  items from dataset\n",
      "Loading:  600  items from dataset\n",
      "Loading:  650  items from dataset\n",
      "Loading:  700  items from dataset\n",
      "Loading:  750  items from dataset\n",
      "Loading:  800  items from dataset\n",
      "Loading:  850  items from dataset\n",
      "Loading:  900  items from dataset\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATASET\n",
    "dataset = dataloader.MusicDataset('Data/genres_original','Data/features_30_sec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET BLOCK\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL - GENERATOR\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=8, conv_dim=64, repeat_num=6):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        c_dim = num_classes\n",
    "        \n",
    "        #upp sampling\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(1+c_dim, conv_dim, kernel_size=(3, 9), padding=(1, 4), bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(curr_dim, curr_dim))\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL - DISKRIMINATOR\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator,self).__init__()\n",
    "        modules = []\n",
    "\n",
    "        modules.append(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(8))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "\n",
    "        modules.append(nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(16))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "\n",
    "        modules.append(nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(32))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "\n",
    "        modules.append(nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(64))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "\n",
    "        modules.append(nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(32))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "        \n",
    "        \n",
    "        modules.append(nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size = 2, stride = 2))\n",
    "        modules.append(nn.BatchNorm2d(16))\n",
    "        modules.append(nn.ReLU(inplace = True))\n",
    "        \n",
    "        modules.append(nn.AdaptiveAvgPool2d(output_size = 1))\n",
    "\n",
    "        self.fc=nn.Linear(in_features=16,out_features=1,bias=True)\n",
    "        \n",
    "        self.fc_classif = nn.Linear(in_features=16, out_features = num_classes,bias=True)\n",
    "        \n",
    "        self.sequence = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.sequence(x)\n",
    "        y = torch.flatten(y,start_dim=1)\n",
    "        predict_fake = nn.Sigmoid()(self.fc(y))\n",
    "        predict_genre = nn.Softmax()(self.fc_classif(y))\n",
    "        return predict_genre, predict_fake\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE INSTANCES\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "discriminator = Discriminator(len(dataset.classes))\n",
    "generator = Generator(num_classes = len(dataset.classes))\n",
    "\n",
    "#LOAD BEST CLASSIFIER\n",
    "#classifier = best_classif_model.Classifier(len(dataset.classes))\n",
    "#PATH = \"model_best.pt\"\n",
    "#classifier.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#DEVICE\n",
    "device = torch.device('cuda')\n",
    "discriminator = discriminator.cuda()\n",
    "generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isprobavanje 1\n",
    "name, spect, label = dataset.__getitem__(1)\n",
    "spect = spect.reshape(1,spect.size(0),spect.size(1),spect.size(2))\n",
    "\n",
    "spect = spect.to(device)\n",
    "\n",
    "y = discriminator(spect)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isprobavanje 2\n",
    "label_onehot = torch.zeros(len(dataset.classes)) \n",
    "label_onehot[label] = 1\n",
    "label_onehot = label_onehot.reshape(1,-1)\n",
    "label_onehot = label_onehot.to(device)\n",
    "y = generator(spect,label_onehot)\n",
    "print(y.shape)\n",
    "print(spect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isprobavanje 3\n",
    "y = y.detach().cpu().numpy().reshape(y.size(2),y.size(3))\n",
    "spect = spect.detach().cpu().numpy().reshape(spect.size(2),spect.size(3))\n",
    "plt.imshow(y)\n",
    "plt.show()\n",
    "plt.imshow(spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "num_workers = 0\n",
    "# make train and valid splits\n",
    "\n",
    "random.seed(0)  # rng seed, set to 0 for reproducibility\n",
    "\n",
    "dataset_indices = list(range(len(dataset)))\n",
    "random.shuffle(dataset_indices)  # shuffle the indices before splitting (use random.shuffle)\n",
    "\n",
    "#split datasets\n",
    "train_split_indices = dataset_indices[:int(len(dataset_indices)*0.8)]  # get the training split indices\n",
    "#valid_split_indices = dataset_indices[int(len(dataset_indices)*0.6):int(len(dataset_indices)*0.8)]  # get the validation split indices \n",
    "#test_split_indices = dataset_indices[int(len(dataset_indices)*0.8):]\n",
    "\n",
    "train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_split_indices)\n",
    "#valid_subset_sampler = torch.utils.data.SubsetRandomSampler(valid_split_indices)\n",
    "#test_subset_sampler = torch.utils.data.SubsetRandomSampler(test_split_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, sampler=train_subset_sampler, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(),lr=learning_rate)\n",
    "d_optimizer = torch.optim.Adam(generator.parameters(),lr=learning_rate)\n",
    "\n",
    "n_disc = 10  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\psiml_gpu\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss disc:  tensor(1.1015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.1005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  tensor(1.0974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss disc:  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a67d82d81f41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss disc: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_disc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\psiml_gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;31m# characters to replace unicode characters with.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\psiml_gpu\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\psiml_gpu\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\psiml_gpu\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_class_d = 0.5 #parameter for the \"importance\" of good classification\n",
    "lambda_class = 0.5\n",
    "lambda_cycle = 0.5\n",
    "lambda_ident = 0\n",
    "#TODO: parametri wtf\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (name,spect,label) in enumerate(train_dataloader):\n",
    "        \n",
    "        #get labels (c) and convert to onehot\n",
    "        label_goal = []\n",
    "        rnd_label = []\n",
    "        label_onehot = []\n",
    "        for j in range(batch_size):\n",
    "            rnd_label.append(random.randint(0,len(dataset.classes)-1))\n",
    "            label_goal.append(np.zeros(len(dataset.classes)))\n",
    "            label_goal[j][rnd_label] = 1\n",
    "            \n",
    "            label_onehot.append(np.zeros(len(dataset.classes)))\n",
    "            label_onehot[j][label[j]] = 1\n",
    "            \n",
    "        label_goal = torch.Tensor(label_goal) \n",
    "        label_onehot = torch.Tensor(label_onehot)\n",
    "        \n",
    "        #print('ucitao labele\\n')\n",
    "        \n",
    "        label = label.type(torch.LongTensor)\n",
    "        rnd_label = torch.Tensor(rnd_label)\n",
    "        rnd_label = rnd_label.type(torch.LongTensor)\n",
    "        \n",
    "        label = label.to(device)\n",
    "        rnd_label = rnd_label.to(device)\n",
    "        label_onehot = label_onehot.to(device)\n",
    "        label_goal = label_goal.to(device)\n",
    "        spect = spect.to(device)\n",
    "        \n",
    "        #print('prebaceno na device\\n')\n",
    "        \n",
    "        #train discriminator\n",
    "        genre,fake = discriminator(spect)\n",
    "        loss_real = - torch.mean(fake)\n",
    "        loss_class = F.cross_entropy(genre, label)\n",
    "        \n",
    "        #print('prosao kroz jedan discr\\n')\n",
    "        \n",
    "        #moguc izvor baga - torch.no_grad kao alternativa\n",
    "        with torch.no_grad():\n",
    "            generator_out = generator(spect,label_goal)\n",
    "            \n",
    "        #print('prosao kroz gen\\n')\n",
    "        genre, fake = discriminator(generator_out)\n",
    "        loss_fake = torch.mean(fake)\n",
    "        \n",
    "        #TODO: add gradient penalty \n",
    "        \n",
    "        loss_disc = loss_real + loss_fake + lambda_class_d * loss_class\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #train generator\n",
    "        if (i % n_disc):\n",
    "            generator_out = generator(spect,label_goal)\n",
    "            genre,fake = discriminator(generator_out)\n",
    "            loss_fake = - torch.mean(fake) #adversary loss\n",
    "            loss_class = F.cross_entropy(genre, rnd_label) #wrong class\n",
    "            \n",
    "            self_inverse = generator(generator_out,label_onehot)\n",
    "            loss_cycle = torch.mean(torch.abs(self_inverse - spect))\n",
    "            \n",
    "            self_out = generator(spect,label_onehot)\n",
    "            loss_ident = torch.mean(torch.abs(self_out-spect))\n",
    "            \n",
    "            loss_gen = loss_fake + lambda_class*loss_class + lambda_cycle * loss_cycle + lambda_ident * loss_ident\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "        if(i%1 == 0):\n",
    "            print('Loss disc: ',loss_disc)\n",
    "            \n",
    "print('Epoch: ',epoch,'\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_goal = label_goal.reshape(1,-1)\n",
    "print(label_goal.shape)\n",
    "print(spect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
